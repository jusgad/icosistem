# =============================================================================
# Prometheus Alerting Rules - Ecosistema de Emprendimiento
# =============================================================================
# 
# Configuraci√≥n completa de alertas para monitoreo integral que incluye:
# - Alertas de aplicaci√≥n Flask y APIs
# - Monitoreo de infraestructura cr√≠tica
# - Alertas de base de datos PostgreSQL
# - Monitoreo de cache Redis
# - Alertas de Celery workers y queues
# - Monitoreo de Nginx y conectividad
# - Alertas de m√©tricas de negocio
# - Alertas de seguridad y performance
# - Alertas de recursos del sistema
# - Configuraciones por severidad y escalamiento
#
# Severidades:
# - critical: Problemas que requieren intervenci√≥n inmediata
# - warning: Problemas que requieren atenci√≥n pero no son cr√≠ticos
# - info: Informaci√≥n para awareness, no requiere acci√≥n inmediata
#
# Versi√≥n: 1.0.0
# Autor: Sistema de Emprendimiento
# Compatible con: Prometheus 2.40+, AlertManager 0.25+
# =============================================================================

groups:
  # ---------------------------------------------------------------------------
  # ALERTAS DE APLICACI√ìN Y SERVICIOS CR√çTICOS
  # ---------------------------------------------------------------------------
  - name: application.rules
    interval: 30s
    rules:
      # Servicio Down - Cr√≠tico
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
          team: platform
          service: "{{ $labels.job }}"
          environment: "{{ $labels.environment }}"
        annotations:
          summary: "üö® Servicio {{ $labels.job }} est√° DOWN"
          description: |
            El servicio {{ $labels.job }} en la instancia {{ $labels.instance }} 
            ha estado inaccesible por m√°s de 1 minuto.
            
            Instancia: {{ $labels.instance }}
            Job: {{ $labels.job }}
            Entorno: {{ $labels.environment }}
            
            Acciones recomendadas:
            1. Verificar logs del servicio
            2. Revisar conectividad de red
            3. Verificar recursos del sistema
            4. Reiniciar servicio si es necesario
          dashboard: "http://grafana:3000/d/ecosistema-emprendimiento-main"
          runbook: "https://docs.ecosistema.com/runbooks/service-down"

      # Rate de Requests HTTP Alto
      - alert: HighHTTPRequestRate
        expr: rate(flask_http_request_total{job="ecosistema-app"}[5m]) > 100
        for: 5m
        labels:
          severity: warning
          team: backend
          service: app
          endpoint: "{{ $labels.endpoint }}"
        annotations:
          summary: "üìä Rate de requests HTTP alto en {{ $labels.endpoint }}"
          description: |
            El endpoint {{ $labels.endpoint }} est√° recibiendo {{ $value }} requests/sec,
            lo cual est√° por encima del umbral normal de 100 req/sec.
            
            Instancia: {{ $labels.instance }}
            Endpoint: {{ $labels.endpoint }}
            Rate actual: {{ $value | humanize }} req/sec
            
            Posibles causas:
            - Pico de tr√°fico leg√≠timo
            - Ataque DDoS
            - Loop infinito en cliente
            - Bot malicioso
            
            Verificar logs de Nginx y aplicaci√≥n.

      # Tasa de Errores HTTP Alta
      - alert: HighHTTPErrorRate
        expr: |
          (
            rate(flask_http_request_total{status=~"5..", job="ecosistema-app"}[5m]) /
            rate(flask_http_request_total{job="ecosistema-app"}[5m])
          ) * 100 > 5
        for: 3m
        labels:
          severity: critical
          team: backend
          service: app
          endpoint: "{{ $labels.endpoint }}"
        annotations:
          summary: "üö® Tasa de errores HTTP alta en {{ $labels.endpoint }}"
          description: |
            El endpoint {{ $labels.endpoint }} tiene una tasa de errores 5xx del {{ $value }}%,
            lo cual est√° por encima del umbral cr√≠tico del 5%.
            
            Instancia: {{ $labels.instance }}
            Endpoint: {{ $labels.endpoint }}
            Tasa de error: {{ $value | humanizePercentage }}
            
            Acciones inmediatas:
            1. Revisar logs de aplicaci√≥n
            2. Verificar conectividad a BD y Redis
            3. Revisar recursos del sistema
            4. Considerar fallback o rollback

      # Latencia Alta de Requests
      - alert: HighRequestLatency
        expr: |
          histogram_quantile(0.95, 
            rate(flask_request_duration_seconds_bucket{job="ecosistema-app"}[5m])
          ) > 2
        for: 5m
        labels:
          severity: warning
          team: backend
          service: app
          endpoint: "{{ $labels.endpoint }}"
        annotations:
          summary: "‚è±Ô∏è Latencia alta en {{ $labels.endpoint }}"
          description: |
            El percentil 95 de latencia para {{ $labels.endpoint }} es {{ $value }}s,
            lo cual est√° por encima del umbral de 2 segundos.
            
            Instancia: {{ $labels.instance }}
            Endpoint: {{ $labels.endpoint }}
            P95 latencia: {{ $value | humanizeDuration }}
            
            Posibles causas:
            - Queries lentas a base de datos
            - Procesamiento CPU intensivo
            - Llamadas externas lentas
            - Falta de recursos

      # Uso Alto de Memoria Python
      - alert: HighPythonMemoryUsage
        expr: process_resident_memory_bytes{job="ecosistema-app"} / 1024 / 1024 > 1024
        for: 10m
        labels:
          severity: warning
          team: backend
          service: app
        annotations:
          summary: "üß† Uso alto de memoria en aplicaci√≥n Python"
          description: |
            La aplicaci√≥n Python est√° usando {{ $value }}MB de memoria,
            lo cual est√° por encima del umbral de 1GB.
            
            Instancia: {{ $labels.instance }}
            Memoria actual: {{ $value | humanize }}MB
            
            Verificar:
            - Memory leaks en c√≥digo
            - Crecimiento de estructuras de datos
            - Garbage collection
            - Configuraci√≥n de workers

      # Application Startup Lento
      - alert: SlowApplicationStartup
        expr: time() - process_start_time_seconds{job="ecosistema-app"} < 300 and on(instance) rate(flask_http_request_total[1m]) == 0
        for: 5m
        labels:
          severity: warning
          team: backend
          service: app
        annotations:
          summary: "üêå Aplicaci√≥n tardando en inicializar"
          description: |
            La aplicaci√≥n en {{ $labels.instance }} se reinici√≥ hace {{ $value | humanizeDuration }}
            pero a√∫n no est√° sirviendo tr√°fico.
            
            Verificar:
            - Logs de inicializaci√≥n
            - Conectividad a dependencias
            - Health checks
            - Configuraci√≥n de timeouts

  # ---------------------------------------------------------------------------
  # ALERTAS DE BASE DE DATOS POSTGRESQL
  # ---------------------------------------------------------------------------
  - name: postgresql.rules
    interval: 30s
    rules:
      # PostgreSQL Down
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          team: database
          service: postgresql
        annotations:
          summary: "üö® PostgreSQL est√° DOWN"
          description: |
            La base de datos PostgreSQL no est√° respondiendo.
            
            Instancia: {{ $labels.instance }}
            
            Acciones cr√≠ticas:
            1. Verificar proceso PostgreSQL
            2. Revisar logs de PostgreSQL
            3. Verificar espacio en disco
            4. Verificar memoria disponible
            5. Activar procedimiento de recuperaci√≥n

      # Conexiones PostgreSQL Altas
      - alert: PostgreSQLHighConnections
        expr: |
          (
            pg_stat_database_numbackends{datname!~"template.*|postgres"} /
            pg_settings_max_connections
          ) * 100 > 80
        for: 5m
        labels:
          severity: warning
          team: database
          service: postgresql
          database: "{{ $labels.datname }}"
        annotations:
          summary: "üîó Conexiones PostgreSQL altas en {{ $labels.datname }}"
          description: |
            La base de datos {{ $labels.datname }} est√° usando {{ $value }}% 
            de las conexiones m√°ximas permitidas.
            
            Base de datos: {{ $labels.datname }}
            Uso actual: {{ $value | humanizePercentage }}
            
            Verificar:
            - Connection pooling
            - Queries lentas que no liberan conexiones
            - Configuraci√≥n de max_connections
            - Procesos zombie

      # Queries Lentas PostgreSQL
      - alert: PostgreSQLLongRunningQueries
        expr: pg_stat_activity_max_tx_duration > 300
        for: 2m
        labels:
          severity: warning
          team: database
          service: postgresql
        annotations:
          summary: "üêå Queries PostgreSQL ejecut√°ndose por mucho tiempo"
          description: |
            Hay queries ejecut√°ndose por m√°s de {{ $value | humanizeDuration }}.
            
            Duraci√≥n m√°xima: {{ $value | humanizeDuration }}
            
            Acciones:
            1. Identificar queries problem√°ticas
            2. Revisar planes de ejecuci√≥n
            3. Considerar optimizaci√≥n de √≠ndices
            4. Evaluar si cancelar queries

      # Deadlocks PostgreSQL
      - alert: PostgreSQLDeadlocks
        expr: increase(pg_stat_database_deadlocks{datname!~"template.*|postgres"}[5m]) > 0
        for: 0m
        labels:
          severity: warning
          team: database
          service: postgresql
          database: "{{ $labels.datname }}"
        annotations:
          summary: "üîí Deadlocks detectados en PostgreSQL"
          description: |
            Se detectaron {{ $value }} deadlocks en los √∫ltimos 5 minutos
            en la base de datos {{ $labels.datname }}.
            
            Base de datos: {{ $labels.datname }}
            Deadlocks recientes: {{ $value }}
            
            Revisar:
            - Orden de acceso a tablas
            - L√≥gica de transacciones
            - √çndices en foreign keys
            - Logs de PostgreSQL

      # Tama√±o de Base de Datos Creciendo R√°pido
      - alert: PostgreSQLDatabaseSizeGrowth
        expr: |
          (
            increase(pg_database_size_bytes{datname!~"template.*|postgres"}[24h]) /
            pg_database_size_bytes{datname!~"template.*|postgres"} offset 24h
          ) * 100 > 20
        for: 1h
        labels:
          severity: warning
          team: database
          service: postgresql
          database: "{{ $labels.datname }}"
        annotations:
          summary: "üìà Crecimiento r√°pido de BD {{ $labels.datname }}"
          description: |
            La base de datos {{ $labels.datname }} creci√≥ {{ $value }}% en las √∫ltimas 24 horas.
            
            Base de datos: {{ $labels.datname }}
            Crecimiento: {{ $value | humanizePercentage }}
            
            Verificar:
            - Logs y auditor√≠a
            - Procesos de limpieza
            - Retenci√≥n de datos
            - Posible data leak

      # Replication Lag Alto
      - alert: PostgreSQLReplicationLag
        expr: pg_stat_replication_lag > 60
        for: 5m
        labels:
          severity: critical
          team: database
          service: postgresql
        annotations:
          summary: "üîÑ Lag de replicaci√≥n PostgreSQL alto"
          description: |
            El lag de replicaci√≥n es {{ $value | humanizeDuration }}.
            
            Lag actual: {{ $value | humanizeDuration }}
            
            Causas posibles:
            - Red lenta entre primary y replica
            - Carga alta en primary
            - Replica con recursos insuficientes
            - Configuraci√≥n incorrecta

  # ---------------------------------------------------------------------------
  # ALERTAS DE REDIS CACHE
  # ---------------------------------------------------------------------------
  - name: redis.rules
    interval: 30s
    rules:
      # Redis Down
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          team: cache
          service: redis
        annotations:
          summary: "üö® Redis est√° DOWN"
          description: |
            El servidor Redis no est√° respondiendo.
            
            Instancia: {{ $labels.instance }}
            
            Impacto:
            - Cache no disponible
            - Sessions de usuario perdidas
            - Performance degradada
            - Celery sin message broker
            
            Acciones inmediatas:
            1. Verificar proceso Redis
            2. Revisar logs de Redis
            3. Verificar memoria disponible
            4. Reiniciar Redis si es necesario

      # Uso Alto de Memoria Redis
      - alert: RedisHighMemoryUsage
        expr: (redis_memory_used_bytes / redis_memory_max_bytes) * 100 > 90
        for: 5m
        labels:
          severity: critical
          team: cache
          service: redis
        annotations:
          summary: "üß† Uso cr√≠tico de memoria en Redis"
          description: |
            Redis est√° usando {{ $value }}% de la memoria m√°xima configurada.
            
            Uso actual: {{ $value | humanizePercentage }}
            Memoria usada: {{ with query "redis_memory_used_bytes" }}{{ . | first | value | humanizeBytes }}{{ end }}
            Memoria m√°xima: {{ with query "redis_memory_max_bytes" }}{{ . | first | value | humanizeBytes }}{{ end }}
            
            Riesgo de:
            - Eviction de keys
            - Performance degradada
            - Out of memory
            
            Acciones:
            1. Revisar pol√≠tica de eviction
            2. Limpiar keys expiradas
            3. Aumentar memoria si es necesario
            4. Revisar uso por aplicaci√≥n

      # Hit Rate Bajo de Redis
      - alert: RedisLowHitRate
        expr: |
          (
            redis_keyspace_hits_total /
            (redis_keyspace_hits_total + redis_keyspace_misses_total)
          ) < 0.8
        for: 10m
        labels:
          severity: warning
          team: cache
          service: redis
        annotations:
          summary: "üéØ Hit rate bajo en Redis"
          description: |
            El hit rate de Redis es {{ $value | humanizePercentage }}, 
            por debajo del umbral del 80%.
            
            Hit rate actual: {{ $value | humanizePercentage }}
            
            Posibles causas:
            - TTL muy corto en keys
            - Patrones de acceso cambiaron
            - Eviction frecuente por memoria
            - Cache warming insuficiente
            
            Optimizar:
            - Estrategia de caching
            - TTL de keys
            - Warm-up procedures

      # Conexiones Rechazadas Redis
      - alert: RedisRejectedConnections
        expr: increase(redis_rejected_connections_total[5m]) > 0
        for: 1m
        labels:
          severity: warning
          team: cache
          service: redis
        annotations:
          summary: "üö´ Redis rechazando conexiones"
          description: |
            Redis rechaz√≥ {{ $value }} conexiones en los √∫ltimos 5 minutos.
            
            Conexiones rechazadas: {{ $value }}
            
            Causas posibles:
            - L√≠mite de conexiones alcanzado
            - Configuraci√≥n maxclients insuficiente
            - Connection pooling inadecuado
            
            Verificar configuraci√≥n y uso de conexiones.

      # Keys Expiradas No Procesadas
      - alert: RedisExpiredKeysHigh
        expr: increase(redis_expired_keys_total[5m]) > 1000
        for: 5m
        labels:
          severity: warning
          team: cache
          service: redis
        annotations:
          summary: "‚è∞ Alto n√∫mero de keys expiradas en Redis"
          description: |
            {{ $value }} keys expiraron en los √∫ltimos 5 minutos.
            
            Keys expiradas: {{ $value }}
            
            Puede indicar:
            - TTL muy corto
            - Alto volumen de datos temporales
            - Necesidad de optimizaci√≥n

  # ---------------------------------------------------------------------------
  # ALERTAS DE CELERY WORKERS
  # ---------------------------------------------------------------------------
  - name: celery.rules
    interval: 30s
    rules:
      # Workers de Celery Down
      - alert: CeleryWorkersDown
        expr: sum(up{job="celery-workers"}) == 0
        for: 2m
        labels:
          severity: critical
          team: backend
          service: celery
        annotations:
          summary: "üö® Todos los workers de Celery est√°n DOWN"
          description: |
            No hay workers de Celery disponibles para procesar tareas.
            
            Workers activos: {{ $value }}
            
            Impacto cr√≠tico:
            - No se procesan tareas as√≠ncronas
            - Emails no se env√≠an
            - Reportes no se generan
            - Notificaciones no funcionan
            
            Acciones inmediatas:
            1. Verificar procesos de Celery
            2. Revisar conectividad a Redis
            3. Verificar logs de workers
            4. Reiniciar workers

      # Pocos Workers Activos
      - alert: CeleryFewWorkersActive
        expr: sum(up{job="celery-workers"}) < 2
        for: 5m
        labels:
          severity: warning
          team: backend
          service: celery
        annotations:
          summary: "‚ö†Ô∏è Pocos workers de Celery activos"
          description: |
            Solo {{ $value }} worker(s) de Celery est√°n activos.
            
            Workers activos: {{ $value }}
            M√≠nimo recomendado: 2
            
            Riesgos:
            - Procesamiento lento de tareas
            - Single point of failure
            - Cola de tareas creciendo
            
            Considerar levantar m√°s workers.

      # Alta Tasa de Tareas Fallidas
      - alert: CeleryHighFailureRate
        expr: |
          (
            rate(celery_tasks_total{state="FAILURE"}[10m]) /
            rate(celery_tasks_total[10m])
          ) * 100 > 10
        for: 5m
        labels:
          severity: warning
          team: backend
          service: celery
          task_name: "{{ $labels.task_name }}"
        annotations:
          summary: "‚ùå Alta tasa de fallos en tareas Celery"
          description: |
            La tarea {{ $labels.task_name }} tiene {{ $value }}% de tasa de fallos.
            
            Tarea: {{ $labels.task_name }}
            Tasa de fallos: {{ $value | humanizePercentage }}
            
            Verificar:
            - Logs de errores de la tarea
            - Dependencias externas
            - Configuraci√≥n de reintentos
            - Datos de entrada v√°lidos

      # Queue de Tareas Creciendo
      - alert: CeleryQueueSizeGrowing
        expr: increase(celery_queue_length[10m]) > 100
        for: 5m
        labels:
          severity: warning
          team: backend
          service: celery
          queue: "{{ $labels.queue }}"
        annotations:
          summary: "üìà Cola de Celery creciendo r√°pidamente"
          description: |
            La cola {{ $labels.queue }} creci√≥ en {{ $value }} tareas en 10 minutos.
            
            Cola: {{ $labels.queue }}
            Crecimiento: {{ $value }} tareas
            
            Posibles causas:
            - Workers insuficientes
            - Tareas lentas
            - Pico de carga
            - Workers con problemas
            
            Considerar escalar workers.

      # Tareas con Tiempo de Ejecuci√≥n Largo
      - alert: CeleryLongRunningTasks
        expr: celery_task_runtime_seconds > 600
        for: 5m
        labels:
          severity: warning
          team: backend
          service: celery
          task_name: "{{ $labels.task_name }}"
        annotations:
          summary: "üïê Tarea Celery ejecut√°ndose por mucho tiempo"
          description: |
            La tarea {{ $labels.task_name }} lleva {{ $value | humanizeDuration }} ejecut√°ndose.
            
            Tarea: {{ $labels.task_name }}
            Tiempo ejecuci√≥n: {{ $value | humanizeDuration }}
            Worker: {{ $labels.instance }}
            
            Verificar si la tarea:
            - Est√° colgada
            - Procesa datos excesivos
            - Tiene dependencies lentas
            - Necesita optimizaci√≥n

  # ---------------------------------------------------------------------------
  # ALERTAS DE NGINX WEB SERVER
  # ---------------------------------------------------------------------------
  - name: nginx.rules
    interval: 30s
    rules:
      # Nginx Down
      - alert: NginxDown
        expr: nginx_up == 0
        for: 1m
        labels:
          severity: critical
          team: infrastructure
          service: nginx
        annotations:
          summary: "üö® Nginx est√° DOWN"
          description: |
            El servidor web Nginx no est√° respondiendo.
            
            Instancia: {{ $labels.instance }}
            
            Impacto cr√≠tico:
            - Sitio web no accesible
            - APIs no disponibles
            - Load balancing no funciona
            
            Acciones inmediatas:
            1. Verificar proceso Nginx
            2. Revisar configuraci√≥n
            3. Verificar logs de error
            4. Reiniciar Nginx

      # Alto N√∫mero de Conexiones Nginx
      - alert: NginxHighConnections
        expr: nginx_connections_active > 1000
        for: 5m
        labels:
          severity: warning
          team: infrastructure
          service: nginx
        annotations:
          summary: "üîó Alto n√∫mero de conexiones en Nginx"
          description: |
            Nginx tiene {{ $value }} conexiones activas.
            
            Conexiones activas: {{ $value }}
            Umbral: 1000
            
            Posibles causas:
            - Pico de tr√°fico
            - Slow clients
            - Ataque DDoS
            - Configuraci√≥n de timeouts
            
            Monitorear tr√°fico y considerar rate limiting.

      # Rate Limiting Activo
      - alert: NginxRateLimitingActive
        expr: increase(nginx_http_requests_total{status="429"}[5m]) > 10
        for: 2m
        labels:
          severity: info
          team: security
          service: nginx
        annotations:
          summary: "üõ°Ô∏è Rate limiting activo en Nginx"
          description: |
            Nginx bloque√≥ {{ $value }} requests por rate limiting en 5 minutos.
            
            Requests bloqueados: {{ $value }}
            
            Puede indicar:
            - Intentos de abuso
            - Cliente mal configurado
            - Bot malicioso
            - Rate limit muy estricto
            
            Revisar logs para identificar origen.

      # Errores 5xx Alto en Nginx
      - alert: NginxHighServerErrors
        expr: |
          (
            rate(nginx_http_requests_total{status=~"5.."}[5m]) /
            rate(nginx_http_requests_total[5m])
          ) * 100 > 5
        for: 3m
        labels:
          severity: critical
          team: infrastructure
          service: nginx
        annotations:
          summary: "üö® Alta tasa de errores 5xx en Nginx"
          description: |
            Nginx est√° sirviendo {{ $value }}% de errores 5xx.
            
            Tasa de errores: {{ $value | humanizePercentage }}
            
            Causas posibles:
            - Backend servers down
            - Timeouts a backend
            - Configuraci√≥n incorrecta
            - Sobrecarga del sistema
            
            Verificar estado de backends y logs.

  # ---------------------------------------------------------------------------
  # ALERTAS DE INFRAESTRUCTURA Y SISTEMA
  # ---------------------------------------------------------------------------
  - name: infrastructure.rules
    interval: 30s
    rules:
      # CPU Alto
      - alert: HighCPUUsage
        expr: |
          100 - (
            avg by (instance) (
              rate(node_cpu_seconds_total{mode="idle"}[5m])
            ) * 100
          ) > 85
        for: 10m
        labels:
          severity: warning
          team: infrastructure
          service: system
        annotations:
          summary: "üñ•Ô∏è Uso alto de CPU en {{ $labels.instance }}"
          description: |
            El servidor {{ $labels.instance }} tiene {{ $value }}% de uso de CPU.
            
            Instancia: {{ $labels.instance }}
            CPU usage: {{ $value | humanizePercentage }}
            
            Verificar:
            - Procesos consumiendo CPU
            - Necesidad de scaling
            - Optimizaci√≥n de c√≥digo
            - Recursos asignados

      # Memoria Alta
      - alert: HighMemoryUsage
        expr: |
          (
            (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) /
            node_memory_MemTotal_bytes
          ) * 100 > 90
        for: 5m
        labels:
          severity: critical
          team: infrastructure
          service: system
        annotations:
          summary: "üß† Uso cr√≠tico de memoria en {{ $labels.instance }}"
          description: |
            El servidor {{ $labels.instance }} est√° usando {{ $value }}% de memoria.
            
            Instancia: {{ $labels.instance }}
            Uso de memoria: {{ $value | humanizePercentage }}
            Memoria total: {{ with query "node_memory_MemTotal_bytes" }}{{ . | first | value | humanizeBytes }}{{ end }}
            Memoria disponible: {{ with query "node_memory_MemAvailable_bytes" }}{{ . | first | value | humanizeBytes }}{{ end }}
            
            Riesgo de OOM killer.
            
            Acciones:
            1. Identificar procesos usando memoria
            2. Considerar restart de servicios
            3. Aumentar memoria si es necesario
            4. Revisar memory leaks

      # Disco Lleno
      - alert: DiskSpaceLow
        expr: |
          (
            (node_filesystem_size_bytes{fstype!="tmpfs"} - node_filesystem_avail_bytes{fstype!="tmpfs"}) /
            node_filesystem_size_bytes{fstype!="tmpfs"}
          ) * 100 > 85
        for: 5m
        labels:
          severity: warning
          team: infrastructure
          service: system
          mountpoint: "{{ $labels.mountpoint }}"
        annotations:
          summary: "üíæ Espacio en disco bajo en {{ $labels.instance }}"
          description: |
            El filesystem {{ $labels.mountpoint }} en {{ $labels.instance }} 
            est√° {{ $value }}% lleno.
            
            Instancia: {{ $labels.instance }}
            Mountpoint: {{ $labels.mountpoint }}
            Uso: {{ $value | humanizePercentage }}
            Espacio total: {{ with query "node_filesystem_size_bytes" }}{{ . | first | value | humanizeBytes }}{{ end }}
            Espacio disponible: {{ with query "node_filesystem_avail_bytes" }}{{ . | first | value | humanizeBytes }}{{ end }}
            
            Acciones:
            1. Limpiar logs antiguos
            2. Limpiar archivos temporales
            3. Rotar backups
            4. Aumentar almacenamiento

      # Disco Cr√≠tico
      - alert: DiskSpaceCritical
        expr: |
          (
            (node_filesystem_size_bytes{fstype!="tmpfs"} - node_filesystem_avail_bytes{fstype!="tmpfs"}) /
            node_filesystem_size_bytes{fstype!="tmpfs"}
          ) * 100 > 95
        for: 1m
        labels:
          severity: critical
          team: infrastructure
          service: system
          mountpoint: "{{ $labels.mountpoint }}"
        annotations:
          summary: "üö® Espacio en disco CR√çTICO en {{ $labels.instance }}"
          description: |
            CR√çTICO: El filesystem {{ $labels.mountpoint }} en {{ $labels.instance }} 
            est√° {{ $value }}% lleno.
            
            Instancia: {{ $labels.instance }}
            Mountpoint: {{ $labels.mountpoint }}
            Uso: {{ $value | humanizePercentage }}
            
            ACCI√ìN INMEDIATA REQUERIDA:
            - Servicios pueden fallar
            - Base de datos puede corromperse
            - Logs pueden perderse
            
            Liberar espacio INMEDIATAMENTE.

      # Load Average Alto
      - alert: HighLoadAverage
        expr: node_load15 / on(instance) count by (instance)(node_cpu_seconds_total{mode="idle"}) > 2
        for: 10m
        labels:
          severity: warning
          team: infrastructure
          service: system
        annotations:
          summary: "‚ö° Load average alto en {{ $labels.instance }}"
          description: |
            El load average de 15 minutos en {{ $labels.instance }} es {{ $value }}.
            
            Instancia: {{ $labels.instance }}
            Load average 15m: {{ $value }}
            CPU cores: {{ with query "count by (instance)(node_cpu_seconds_total{mode=\"idle\"})" }}{{ . | first | value }}{{ end }}
            
            Indica sistema sobrecargado.
            Verificar procesos y considerar scaling.

      # Reinicio del Sistema
      - alert: SystemRestarted
        expr: time() - node_boot_time_seconds < 600
        for: 0m
        labels:
          severity: info
          team: infrastructure
          service: system
        annotations:
          summary: "üîÑ Sistema {{ $labels.instance }} reiniciado"
          description: |
            El servidor {{ $labels.instance }} se reinici√≥ hace {{ $value | humanizeDuration }}.
            
            Instancia: {{ $labels.instance }}
            Boot time: {{ $value | humanizeDuration }} ago
            
            Verificar:
            - Causa del reinicio
            - Todos los servicios iniciaron
            - Health checks pasando
            - Logs del sistema

  # ---------------------------------------------------------------------------
  # ALERTAS DE M√âTRICAS DE NEGOCIO
  # ---------------------------------------------------------------------------
  - name: business.rules
    interval: 60s
    rules:
      # Registros de Usuario Bajos
      - alert: LowUserRegistrations
        expr: increase(ecosistema_user_registrations_total[1h]) < 5
        for: 2h
        labels:
          severity: warning
          team: business
          service: app
        annotations:
          summary: "üìâ Registros de usuarios bajos"
          description: |
            Solo {{ $value }} usuarios se registraron en la √∫ltima hora.
            
            Registros √∫ltima hora: {{ $value }}
            Umbral m√≠nimo: 5
            
            Posibles causas:
            - Problemas en funnel de registro
            - Issues en formularios
            - Problemas de UX
            - Baja en tr√°fico org√°nico
            - Campa√±as de marketing pausadas
            
            Revisar analytics y user journey.

      # Creaci√≥n de Proyectos Baja
      - alert: LowProjectCreations
        expr: increase(ecosistema_projects_created_total[24h]) < 10
        for: 4h
        labels:
          severity: warning
          team: business
          service: app
        annotations:
          summary: "üöÄ Creaci√≥n de proyectos baja"
          description: |
            Solo {{ $value }} proyectos creados en las √∫ltimas 24 horas.
            
            Proyectos √∫ltimas 24h: {{ $value }}
            Umbral m√≠nimo: 10
            
            Puede indicar:
            - Problemas en onboarding
            - UX compleja para crear proyectos
            - Falta de engagement
            - Issues t√©cnicos
            
            Revisar flujo de creaci√≥n de proyectos.

      # Tasa de Conversi√≥n Baja
      - alert: LowConversionRate
        expr: ecosistema_conversion_rate < 0.03
        for: 1h
        labels:
          severity: warning
          team: business
          service: app
        annotations:
          summary: "üìà Tasa de conversi√≥n baja"
          description: |
            La tasa de conversi√≥n actual es {{ $value | humanizePercentage }}.
            
            Conversi√≥n actual: {{ $value | humanizePercentage }}
            Umbral m√≠nimo: 3%
            
            Optimizar:
            - Landing pages
            - Call-to-actions
            - User onboarding
            - Value proposition
            - A/B testing

      # Revenue Diario Bajo
      - alert: LowDailyRevenue
        expr: ecosistema_daily_revenue < 1000
        for: 6h
        labels:
          severity: critical
          team: business
          service: app
        annotations:
          summary: "üí∞ Revenue diario bajo"
          description: |
            El revenue diario actual es ${{ $value }}.
            
            Revenue actual: ${{ $value }}
            Objetivo m√≠nimo: $1000
            
            Requiere atenci√≥n inmediata:
            - Revisar flujos de pago
            - Verificar integraciones de payment
            - Analizar abandono de carritos
            - Revisar pricing strategy
            - Contactar team de sales

      # Usuarios Activos Bajo
      - alert: LowActiveUsers
        expr: ecosistema_active_users_current < 50
        for: 30m
        labels:
          severity: warning
          team: business
          service: app
        annotations:
          summary: "üë• Pocos usuarios activos"
          description: |
            Solo {{ $value }} usuarios est√°n activos actualmente.
            
            Usuarios activos: {{ $value }}
            Umbral m√≠nimo: 50
            
            Puede indicar:
            - Horario de baja actividad natural
            - Problemas de performance
            - Issues de UX
            - Problemas de conectividad
            
            Analizar patrones de uso hist√≥rico.

      # Spike Anormal en Registros
      - alert: UnusualRegistrationSpike
        expr: |
          increase(ecosistema_user_registrations_total[10m]) >
          (avg_over_time(increase(ecosistema_user_registrations_total[10m])[1h:10m]) * 3)
        for: 5m
        labels:
          severity: info
          team: business
          service: app
        annotations:
          summary: "üìà Spike inusual en registros"
          description: |
            Hay {{ $value }} registros en los √∫ltimos 10 minutos,
            3x m√°s que el promedio hist√≥rico.
            
            Registros √∫ltimos 10m: {{ $value }}
            
            Puede ser:
            - Viral marketing success
            - Menci√≥n en medios
            - Bot activity
            - Campa√±a exitosa
            
            Verificar origen del tr√°fico y calidad.

  # ---------------------------------------------------------------------------
  # ALERTAS DE SEGURIDAD
  # ---------------------------------------------------------------------------
  - name: security.rules
    interval: 30s
    rules:
      # M√∫ltiples Intentos de Login Fallidos
      - alert: HighFailedLogins
        expr: increase(flask_http_request_total{endpoint="/auth/login", status="401"}[5m]) > 20
        for: 2m
        labels:
          severity: warning
          team: security
          service: app
        annotations:
          summary: "üõ°Ô∏è M√∫ltiples intentos de login fallidos"
          description: |
            {{ $value }} intentos de login fallidos en los √∫ltimos 5 minutos.
            
            Intentos fallidos: {{ $value }}
            
            Posibles indicios de:
            - Ataque de fuerza bruta
            - Credential stuffing
            - Usuario leg√≠timo con password olvidado
            
            Revisar:
            - IPs de origen
            - Patrones de acceso
            - Implementar rate limiting
            - Considerar CAPTCHA

      # Requests a Endpoints No Existentes
      - alert: High404Errors
        expr: increase(nginx_http_requests_total{status="404"}[10m]) > 100
        for: 5m
        labels:
          severity: warning
          team: security
          service: nginx
        annotations:
          summary: "üîç Alto n√∫mero de errores 404"
          description: |
            {{ $value }} requests a URLs no existentes en 10 minutos.
            
            Requests 404: {{ $value }}
            
            Puede indicar:
            - Reconocimiento/scanning
            - Broken links
            - Bot malicioso
            - Referencias incorrectas
            
            Revisar logs para identificar patrones.

      # Requests con User-Agent Sospechoso
      - alert: SuspiciousUserAgents
        expr: increase(nginx_http_requests_total{user_agent=~".*bot.*|.*crawler.*|.*scanner.*"}[10m]) > 200
        for: 5m
        labels:
          severity: info
          team: security
          service: nginx
        annotations:
          summary: "ü§ñ Alto tr√°fico de bots detectado"
          description: |
            {{ $value }} requests de bots/crawlers en 10 minutos.
            
            Bot requests: {{ $value }}
            
            Verificar si son:
            - Bots leg√≠timos (Google, etc.)
            - Scrapers no autorizados
            - Security scanners
            - Load testing
            
            Considerar rate limiting por user-agent.

      # Acceso a Archivos Sensibles
      - alert: SensitiveFileAccess
        expr: increase(nginx_http_requests_total{request=~".*(config|env|backup|sql|log).*"}[5m]) > 0
        for: 0m
        labels:
          severity: warning
          team: security
          service: nginx
        annotations:
          summary: "‚ö†Ô∏è Intento de acceso a archivos sensibles"
          description: |
            {{ $value }} intentos de acceso a archivos sensibles detectados.
            
            Intentos: {{ $value }}
            
            Archivos objetivo pueden incluir:
            - Archivos de configuraci√≥n
            - Variables de entorno
            - Backups de BD
            - Logs del sistema
            
            REVISAR INMEDIATAMENTE logs detallados.

      # SQL Injection Attempts
      - alert: SQLInjectionAttempts
        expr: increase(nginx_http_requests_total{request=~".*union.*select.*|.*drop.*table.*|.*insert.*into.*"}[10m]) > 0
        for: 0m
        labels:
          severity: critical
          team: security
          service: nginx
        annotations:
          summary: "üö® Intentos de SQL Injection detectados"
          description: |
            {{ $value }} posibles intentos de SQL injection en 10 minutos.
            
            Intentos detectados: {{ $value }}
            
            ACCI√ìN INMEDIATA:
            1. Bloquear IPs de origen
            2. Revisar logs detallados
            3. Verificar queries a BD
            4. Revisar prepared statements
            5. Considerar WAF
            
            CR√çTICO: Verificar integridad de datos.

  # ---------------------------------------------------------------------------
  # ALERTAS DE CONECTIVIDAD Y NETWORKING
  # ---------------------------------------------------------------------------
  - name: networking.rules
    interval: 30s
    rules:
      # Conectividad HTTP Fallando
      - alert: HTTPConnectivityFailed
        expr: probe_success{job="blackbox-http"} == 0
        for: 1m
        labels:
          severity: critical
          team: infrastructure
          service: connectivity
          target: "{{ $labels.instance }}"
        annotations:
          summary: "üåê Conectividad HTTP fallando a {{ $labels.instance }}"
          description: |
            No se puede conectar v√≠a HTTP a {{ $labels.instance }}.
            
            Target: {{ $labels.instance }}
            
            Verificar:
            - Servicio ejecut√°ndose
            - Configuraci√≥n de firewall
            - DNS resolution
            - SSL certificates
            - Load balancer health

      # Conectividad TCP Fallando
      - alert: TCPConnectivityFailed
        expr: probe_success{job="blackbox-tcp"} == 0
        for: 1m
        labels:
          severity: critical
          team: infrastructure
          service: connectivity
          target: "{{ $labels.instance }}"
        annotations:
          summary: "üîå Conectividad TCP fallando a {{ $labels.instance }}"
          description: |
            No se puede conectar v√≠a TCP a {{ $labels.instance }}.
            
            Target: {{ $labels.instance }}
            
            Verificar:
            - Puerto abierto
            - Servicio escuchando
            - Firewall rules
            - Network routing
            - Security groups

      # SSL Certificate Expirando
      - alert: SSLCertificateExpiring
        expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 7
        for: 0m
        labels:
          severity: warning
          team: infrastructure
          service: ssl
          target: "{{ $labels.instance }}"
        annotations:
          summary: "üîí Certificado SSL expirando en {{ $labels.instance }}"
          description: |
            El certificado SSL para {{ $labels.instance }} expira en {{ $value | humanizeDuration }}.
            
            Target: {{ $labels.instance }}
            Expira en: {{ $value | humanizeDuration }}
            
            Acciones:
            1. Renovar certificado
            2. Actualizar configuraci√≥n
            3. Verificar auto-renewal
            4. Notificar a stakeholders

      # Latencia de Red Alta
      - alert: HighNetworkLatency
        expr: probe_duration_seconds{job="blackbox-http"} > 2
        for: 5m
        labels:
          severity: warning
          team: infrastructure
          service: networking
          target: "{{ $labels.instance }}"
        annotations:
          summary: "üêå Latencia alta a {{ $labels.instance }}"
          description: |
            La latencia a {{ $labels.instance }} es {{ $value | humanizeDuration }}.
            
            Target: {{ $labels.instance }}
            Latencia: {{ $value | humanizeDuration }}
            
            Posibles causas:
            - Congesti√≥n de red
            - Routing sub√≥ptimo
            - Sobrecarga del servidor
            - ISP issues
            
            Verificar rutas de red y performance.

# =============================================================================
# CONFIGURACI√ìN DE ESCALAMIENTO Y NOTIFICACIONES
# =============================================================================
# 
# Las alertas est√°n configuradas con diferentes niveles de severidad:
# 
# CRITICAL (cr√≠tico):
# - Requiere intervenci√≥n inmediata 24/7
# - Afecta disponibilidad del servicio
# - Puede causar p√©rdida de datos
# - Escalamiento: PagerDuty, SMS, llamadas
# 
# WARNING (advertencia):  
# - Requiere atenci√≥n durante horas de trabajo
# - Puede afectar performance
# - Riesgo de escalada a cr√≠tico
# - Escalamiento: Email, Slack, dashboard
# 
# INFO (informaci√≥n):
# - Awareness, no acci√≥n inmediata
# - Trends y patrones
# - Business intelligence
# - Escalamiento: Dashboard, reports
# 
# =============================================================================
# 
# CONFIGURACI√ìN RECOMENDADA DE ALERTMANAGER:
# 
# route:
#   group_by: ['alertname', 'cluster', 'service']
#   group_wait: 10s
#   group_interval: 10s
#   repeat_interval: 1h
#   receiver: 'web.hook'
#   routes:
#   - match:
#       severity: critical
#     receiver: 'pagerduty'
#     group_wait: 5s
#     repeat_interval: 30m
#   - match:
#       severity: warning  
#     receiver: 'slack'
#     group_wait: 30s
#     repeat_interval: 2h
#   - match:
#       severity: info
#     receiver: 'dashboard'
#     group_wait: 5m
#     repeat_interval: 12h
# 
# =============================================================================